{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DV4BCOohybyi"
   },
   "outputs": [],
   "source": [
    "! pip install datasets\n",
    "! pip install livelossplot\n",
    "! pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "45i15DCJ_766"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(\"gpu?\", torch.cuda.is_available())\n",
    "\n",
    "for i in range(torch.cuda.device_count()): print(torch.cuda.get_device_properties(i).name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IPizDs83ALCU"
   },
   "source": [
    "# Choosing architectures for the task\n",
    "\n",
    "We fix the backbone's parameter count to 0.1M, by setting hidden dimension of 64 features and varying depth (=layer count)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QTc5dce9-o47"
   },
   "outputs": [],
   "source": [
    "from generic_classifier import GenericClassifier\n",
    "from utils import print_num_params\n",
    "from models.lstm import LSTMEncoder\n",
    "from models.transformer import TransformerEncoder\n",
    "from models.s4 import S4Encoder\n",
    "\n",
    "lstm_cfg = {'hidden_dim': 64, 'num_layers': 3, 'encoder_module': LSTMEncoder}\n",
    "# print(lstm_cfg)\n",
    "# print_num_params(LSTMEncoder(**lstm_cfg))\n",
    "\n",
    "transformer_cfg = {'hidden_dim': 64, 'num_layers': 2, 'encoder_module': TransformerEncoder}\n",
    "# print(transformer_cfg)\n",
    "# print_num_params(TransformerEncoder(**transformer_cfg))\n",
    "\n",
    "\n",
    "s4_cfg = {'hidden_dim': 64, 'num_layers': 6, 'encoder_module': S4Encoder}\n",
    "# print(s4_cfg)\n",
    "# print_num_params(S4Encoder(**s4_cfg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "We fix the training configuration across experiments such that each training is limited to one hour (regardless to epoch count), and such that the batch-size is identical (although naturally memory consumption differs between models)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters:\n",
    "\n",
    "BATCH_SIZE = 512  # batch-size that fits all models' training to <80GB VRAM (A100)\n",
    "\n",
    "# for training:\n",
    "TRAIN_TIME_LIMIT = 4000  # seconds to limit the train time, due to colab limit\n",
    "NUM_EPOCHS = 1000  # set maximal amount of epoch (mostly time limit will come first)\n",
    "\n",
    "# for pretraining->fine-tuning setting:\n",
    "TIMELIMIT_PT, TIMELIMIT_FT = 2800, 1200\n",
    "NUM_EPOCHS_PT, NUM_EPOCHS_FT = NUM_EPOCHS, NUM_EPOCHS  # set maximal amount of epoch (mostly time limit will come first, so this irrelevant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MZZwRXGk-vCV"
   },
   "source": [
    "# Experiment 1: Training on ListOps (only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jK-PsNLDAgKa"
   },
   "outputs": [],
   "source": [
    "from experiments import setting_1__directly_on_listops\n",
    "\n",
    "\n",
    "exp1_model2metric = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MLneeSZlA7DD"
   },
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y9aP4SFDxleH"
   },
   "outputs": [],
   "source": [
    "model, test_acc = setting_1__directly_on_listops(\n",
    "   model_cls=GenericClassifier,\n",
    "   model_kwargs=lstm_cfg,\n",
    "   batch_size=BATCH_SIZE,\n",
    "   num_epochs=NUM_EPOCHS,\n",
    "   train_time_limit_secs=TRAIN_TIME_LIMIT\n",
    ")\n",
    "\n",
    "exp1_model2metric['lstm'] = test_acc\n",
    "\n",
    "# [OPTIONAL FOR CLEANING VRAM:]\n",
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d_8ZWpFCBUfE"
   },
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G2UV1a5lBUml"
   },
   "outputs": [],
   "source": [
    "model, test_acc = setting_1__directly_on_listops(\n",
    "    model_cls=GenericClassifier,\n",
    "    model_kwargs=transformer_cfg,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    train_time_limit_secs=TRAIN_TIME_LIMIT\n",
    ")\n",
    "\n",
    "exp1_model2metric['transformer'] = test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LpCCSSraBU5c"
   },
   "source": [
    "## S4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yEhbjRuNBVA9"
   },
   "outputs": [],
   "source": [
    "model, test_acc = setting_1__directly_on_listops(\n",
    "    model_cls=GenericClassifier,\n",
    "    model_kwargs=s4_cfg,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    train_time_limit_secs=TRAIN_TIME_LIMIT\n",
    ")\n",
    "\n",
    "exp1_model2metric['s4'] = test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp1_model2metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gKtoIYXYDBLF"
   },
   "source": [
    "# Experiment 2: PreTrain (CLM) on wikitext -> fintune on ListOps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-iHGhZtKDBLG"
   },
   "outputs": [],
   "source": [
    "from experiments import setting_2__clm_pretrain_text_then_listops\n",
    "\n",
    "exp2_model2metric = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qNCwSA_jDBLG"
   },
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Spmv8gDfDBLG"
   },
   "outputs": [],
   "source": [
    "model, test_acc = setting_2__clm_pretrain_text_then_listops(\n",
    "    model_cls=GenericClassifier,\n",
    "    model_kwargs=lstm_cfg,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_epochs_pt=NUM_EPOCHS_PT,\n",
    "    num_epochs_ft=NUM_EPOCHS_FT,\n",
    "    timelimit_pt=TIMELIMIT_PT, timelimit_ft=TIMELIMIT_FT,\n",
    ")\n",
    "\n",
    "exp2_model2metric['lstm'] = test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "txHycFKWDBLH"
   },
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TXSyfS8HDBLI"
   },
   "outputs": [],
   "source": [
    "model, test_acc = setting_2__clm_pretrain_text_then_listops(\n",
    "    model_cls=GenericClassifier,\n",
    "    model_kwargs=transformer_cfg,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_epochs_pt=NUM_EPOCHS_PT, num_epochs_ft=NUM_EPOCHS_FT,\n",
    "    timelimit_pt=TIMELIMIT_PT, timelimit_ft=TIMELIMIT_FT,\n",
    ")\n",
    "\n",
    "exp2_model2metric['transformer'] = test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V4SC_b3FDBLI"
   },
   "source": [
    "## S4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UGk43zRsDBLJ"
   },
   "outputs": [],
   "source": [
    "model, test_acc = setting_2__clm_pretrain_text_then_listops(\n",
    "    model_cls=GenericClassifier,\n",
    "    model_kwargs=s4_cfg,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_epochs_pt=NUM_EPOCHS_PT, num_epochs_ft=NUM_EPOCHS_FT,\n",
    "    timelimit_pt=TIMELIMIT_PT, timelimit_ft=TIMELIMIT_FT,\n",
    ")\n",
    "\n",
    "exp2_model2metric['s4'] = test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp2_model2metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yUuYrMyRDHGb"
   },
   "source": [
    "# Experiment 3: PreTrain (CLM) on ListOps -> fintune on ListOps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C47IwTQkDHGc"
   },
   "outputs": [],
   "source": [
    "from experiments import setting_3__clm_pretrain_listops_then_listops\n",
    "\n",
    "exp3_model2metric = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oA5u_wQ0DHGc"
   },
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gj14IfNGDHGc"
   },
   "outputs": [],
   "source": [
    "model, test_acc = setting_3__clm_pretrain_listops_then_listops(\n",
    "    model_cls=GenericClassifier,\n",
    "    model_kwargs=lstm_cfg,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_epochs_pt=NUM_EPOCHS_PT,\n",
    "    num_epochs_ft=NUM_EPOCHS_FT,\n",
    "    timelimit_pt=TIMELIMIT_PT, timelimit_ft=TIMELIMIT_FT,\n",
    ")\n",
    "\n",
    "exp3_model2metric['lstm'] = test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2eXddqVzDHGc"
   },
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6TlEJbTaDHGd"
   },
   "outputs": [],
   "source": [
    "model, test_acc = setting_3__clm_pretrain_listops_then_listops(\n",
    "    model_cls=GenericClassifier,\n",
    "    model_kwargs=transformer_cfg,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_epochs_pt=NUM_EPOCHS_PT,\n",
    "    num_epochs_ft=NUM_EPOCHS_FT,\n",
    "    timelimit_pt=TIMELIMIT_PT, timelimit_ft=TIMELIMIT_FT,\n",
    ")\n",
    "\n",
    "exp3_model2metric['transformer'] = test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2WXrHu7NDHGd"
   },
   "source": [
    "## S4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BC7Rsrv7DHGz"
   },
   "outputs": [],
   "source": [
    "model, test_acc = setting_3__clm_pretrain_listops_then_listops(\n",
    "    model_cls=GenericClassifier,\n",
    "    model_kwargs=s4_cfg,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_epochs_pt=NUM_EPOCHS,\n",
    "    num_epochs_ft=NUM_EPOCHS,\n",
    "    timelimit_pt=TIMELIMIT_PT, timelimit_ft=TIMELIMIT_FT,\n",
    ")\n",
    "\n",
    "exp3_model2metric['s4'] = test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp3_model2metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "**The following table presents the accuracy on the test-set of ListOps per model, per training setting.**\n",
    "\n",
    "`ListOps_CLS` for training on ListOPs classification, `ListOps_AUT` for training autoregressively (causal language model) on ListOps, `Wikitext_AUT` for traiing in the same manner as the latter but on WikiText dataset. `{X}->{Y}` indicates a pretraining on the first task (`X`) and finetuning on the second (`Y`).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "rows = []\n",
    "for exp_name, result_dict in zip(\n",
    "        ['ListOps_CLS', 'Wikitext_AUT->ListOps_CLS', 'ListOps_AUT->ListOps_CLS'],\n",
    "        [exp1_model2metric, exp2_model2metric, exp3_model2metric]\n",
    "    ):\n",
    "    for model, metric in result_dict.items():\n",
    "        rows.append([exp_name, model, metric])\n",
    "\n",
    "print(tabulate(rows, headers=['training', 'model', 'test_acc']))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMbMX4DEhrSJpRxreCaO1jD",
   "collapsed_sections": [
    "IPizDs83ALCU",
    "MZZwRXGk-vCV",
    "gKtoIYXYDBLF",
    "yUuYrMyRDHGb",
    "2eXddqVzDHGc",
    "2WXrHu7NDHGd"
   ],
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "12K3ASzasSF9K8FMysoV4fLr9sPgeJk3A",
     "timestamp": 1720253442927
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
